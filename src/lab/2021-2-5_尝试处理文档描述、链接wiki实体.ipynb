{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_file_path = r'C:\\workspace\\SOworkspace\\data\\apidoc_description\\javadoc_descriptions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(desc_file_path, 'r', encoding='utf-8') as rf:\n",
    "    descriptions = json.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('api/java.base/java/io/File.html#setExecutable(boolean)',\n",
       " '<div class=\"block\">A convenience method to set the owner\\'s execute permission for this\\n abstract pathname. On some platforms it may be possible to start the Java\\n virtual machine with special privileges that allow it to execute files\\n that are not marked executable.\\n\\n <p>An invocation of this method of the form <code>file.setExcutable(arg)</code>\\n behaves in exactly the same way as the invocation\\n\\n </p><pre><code>\\n     file.setExecutable(arg, true)\\n </code></pre></div>')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "list(descriptions.items())[1235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fudan_graph_path = r'C:\\workspace\\SOworkspace\\data\\concept_map\\fudan_jdk_graph.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "networkx.classes.reportviews.NodeView"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "fudan_graph = nx.read_gpickle(fudan_graph_path)\n",
    "type(fudan_graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 11314,\n",
       " 'properties': {'qualified_name': 'java.io.File.setExecutable(boolean)',\n",
       "  'entity_category': 11,\n",
       "  'full_declaration': 'public boolean setExecutable(boolean executable)',\n",
       "  'api_type': 11,\n",
       "  'short_description': \"A convenience method to set the owner's execute permission for this abstract pathname. On some platforms it may be possible to start the Java virtual machine with special privileges that allow it to execute files that are not marked executable.\\nAn invocation of this method of the form file.setExcutable(arg) behaves in exactly the same way as the invocation\\nfile.setExecutable(arg, true) .\",\n",
       "  'id': 15938,\n",
       "  'added_in_version': '1.6',\n",
       "  'alias': ['File.setExecutable', 'setExecutable', 'set Executable']},\n",
       " 'labels': {'code_element', 'entity', 'jdk8', 'method'}}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "fudan_graph.nodes[11314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[11314]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "file_renameto_method_node = [node for node in fudan_graph.nodes if 'properties' in fudan_graph.nodes[node].keys() and 'qualified_name' in fudan_graph.nodes[node]['properties'].keys() and fudan_graph.nodes[node]['properties']['qualified_name'] == 'java.io.File.setExecutable(boolean)']\n",
    "file_renameto_method_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'annotation class',\n",
       " 'base override method',\n",
       " 'class',\n",
       " 'class type',\n",
       " 'code_element',\n",
       " 'construct method',\n",
       " 'domain term',\n",
       " 'entity',\n",
       " 'enum class',\n",
       " 'enum constants',\n",
       " 'error class',\n",
       " 'exception class',\n",
       " 'field of class',\n",
       " 'interface',\n",
       " 'jdk8',\n",
       " 'method',\n",
       " 'operation',\n",
       " 'package',\n",
       " 'parameter',\n",
       " 'primary type',\n",
       " 'return value',\n",
       " 'sentence',\n",
       " 'type',\n",
       " 'unknown',\n",
       " 'value',\n",
       " 'wikidata'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "label_set = set()\n",
    "for node in fudan_graph.nodes:\n",
    "    if 'labels' in fudan_graph.nodes[node].keys():\n",
    "        label_set.update(fudan_graph.nodes[node]['labels'])\n",
    "label_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[233720,\n",
       " 233721,\n",
       " 233722,\n",
       " 233723,\n",
       " 233724,\n",
       " 233725,\n",
       " 233726,\n",
       " 233727,\n",
       " 233728,\n",
       " 233729]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "term_nodes = [node for node in fudan_graph.nodes if 'domain term' in fudan_graph.nodes[node]['labels']]\n",
    "term_nodes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 233724,\n",
       " 'properties': {'term_name': 'newref', 'alias': {'newref'}, 'lemma': 'newref'},\n",
       " 'labels': {'domain term', 'jdk8'}}"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "fudan_graph.nodes[233724]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<div class=\"block\">A convenience method to set the owner\\'s execute permission for this\\n abstract pathname. On some platforms it may be possible to start the Java\\n virtual machine with special privileges that allow it to execute files\\n that are not marked executable.\\n\\n <p>An invocation of this method of the form <code>file.setExcutable(arg)</code>\\n behaves in exactly the same way as the invocation\\n\\n </p><pre><code>\\n     file.setExecutable(arg, true)\\n </code></pre></div>'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "desc = descriptions['api/java.base/java/io/File.html#setExecutable(boolean)']\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<html><body><div class=\"block\">A convenience method to set the owner's execute permission for this\n",
       " abstract pathname. On some platforms it may be possible to start the Java\n",
       " virtual machine with special privileges that allow it to execute files\n",
       " that are not marked executable.\n",
       "\n",
       " <p>An invocation of this method of the form <code>file.setExcutable(arg)</code>\n",
       " behaves in exactly the same way as the invocation\n",
       "\n",
       " </p></div></body></html>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "soup = BeautifulSoup(desc, 'lxml')\n",
    "for pre in soup.find_all('pre'):\n",
    "    pre.extract()\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"A convenience method to set the owner's execute permission for this abstract pathname.\",\n",
       " 'On some platforms it may be possible to start the Java virtual machine with special privileges that allow it to execute files that are not marked executable.',\n",
       " 'An invocation of this method of the form file.setExcutable(arg) behaves in exactly the same way as the invocation']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "pat = re.compile('<[^>]+>', re.S)\n",
    "sentences = pat.sub('', soup.text)\n",
    "sentences = [' '.join(sent.split()) for sent in sent_tokenize(sentences)]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyNLPFactory:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    __domain_extractor_nlp = None\n",
    "    __identifier_extractor_nlp = None\n",
    "    __simple_nlp = None\n",
    "\n",
    "    @classmethod\n",
    "    def create_spacy_nlp_for_domain_extractor(clss):\n",
    "        \"\"\"\n",
    "        load a spacy nlp pipeline for extract domain entity and relations\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if clss.__domain_extractor_nlp is not None:\n",
    "            return clss.__domain_extractor_nlp\n",
    "\n",
    "        # todo: fix this, write a class as Spacy Component\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        id_re = re.compile(r\"id|ID|Id\")\n",
    "\n",
    "        prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "        infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "        suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "        nlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                                  infix_finditer=infix_re.finditer,\n",
    "                                                  suffix_search=suffix_re.search, token_match=id_re.match)\n",
    "\n",
    "        clss.__domain_extractor_nlp = nlp\n",
    "        return nlp\n",
    "\n",
    "    @classmethod\n",
    "    def create_spacy_nlp_for_identifier_extractor(clss):\n",
    "        \"\"\"\n",
    "        load a spacy nlp pipeline for extract domain entity and relations\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if clss.__identifier_extractor_nlp is not None:\n",
    "            return clss.__identifier_extractor_nlp\n",
    "\n",
    "        # todo: fix this, write a class as Spacy Component\n",
    "        nlp = spacy.load(\"en\")\n",
    "        hyphen_re = re.compile(r\"[A-Za-z\\d]+-[A-Za-z\\d]+|'[a-z]+|''\")\n",
    "\n",
    "        prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "        infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "        suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "        nlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                                  infix_finditer=infix_re.finditer,\n",
    "                                                  suffix_search=suffix_re.search, token_match=hyphen_re.match)\n",
    "\n",
    "        clss.__identifier_extractor_nlp = nlp\n",
    "        return nlp\n",
    "\n",
    "    @classmethod\n",
    "    def create_simple_nlp_pipeline(clss):\n",
    "        \"\"\"\n",
    "        create a simple nlp pipeline, without NER and dependency parser, could tokenize and pos,lemma, will be very fast.        :return:\n",
    "        \"\"\"\n",
    "        if clss.__simple_nlp is not None:\n",
    "            return clss.__simple_nlp\n",
    "\n",
    "        NLP = spacy.load('en', disable=[\"ner\", \"parser\"])\n",
    "        hyphen_re = re.compile(r\"[A-Za-z\\d]+-[A-Za-z\\d]+|'[a-z]+|''\")\n",
    "        prefix_re = spacy.util.compile_prefix_regex(NLP.Defaults.prefixes)\n",
    "        infix_re = spacy.util.compile_infix_regex(NLP.Defaults.infixes)\n",
    "        suffix_re = spacy.util.compile_suffix_regex(NLP.Defaults.suffixes)\n",
    "        NLP.tokenizer = spacy.tokenizer.Tokenizer(NLP.vocab, prefix_search=prefix_re.search,\n",
    "                                                  infix_finditer=infix_re.finditer,\n",
    "                                                  suffix_search=suffix_re.search, token_match=hyphen_re.match)\n",
    "\n",
    "        clss.__simple_nlp = NLP\n",
    "        return NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeElementNameUtil:\n",
    "    PATTERN_2_4 = re.compile(r'([A-Za-z])([24])([A-CE-Za-ce-z])')\n",
    "    PATTERN_split = re.compile(r'([A-Z]+)([A-Z][a-z0-9]+)')\n",
    "    PATTERN_split_num = re.compile(r'([0-9]?[A-Z]+)')\n",
    "\n",
    "    def get_simple_name_with_parent(self, name):\n",
    "        if not name:\n",
    "            return None\n",
    "        team_name = name.split(\"(\")[0]\n",
    "        split_names = team_name.split(\".\")\n",
    "        if len(split_names) <= 1:\n",
    "            return split_names[-1]\n",
    "\n",
    "        child = split_names[-1].strip()\n",
    "        parent = split_names[-2].strip()\n",
    "\n",
    "        return parent + \".\" + child\n",
    "\n",
    "    def simplify(self, name):\n",
    "        \"\"\"\n",
    "        get the simple name for class, method, field, eg. java.util.ArrayList->ArrayList\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        team_name = name.split(\"(\")[0]\n",
    "        simple_name = team_name.split(\".\")[-1].strip()\n",
    "\n",
    "        return simple_name\n",
    "\n",
    "    def uncamelize_from_simple_name(self, name):\n",
    "        \"\"\"\n",
    "        uncamel from simple name of one name, rg. java.util.ArrayList->Array List\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        simple_name = self.simplify(name)\n",
    "\n",
    "        return self.uncamelize(simple_name)\n",
    "\n",
    "    def uncamelize(self, name):\n",
    "        \"\"\"\n",
    "        uncamel one name\n",
    "        :param name: the camel styple name(include underline)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        # sub = re.sub(r'([A-Za-z])([24])([A-CE-Za-ce-z])', r'\\1 \\2 \\3', name).strip()\n",
    "        sub = re.sub(self.PATTERN_2_4, r'\\1 \\2 \\3', name).strip()\n",
    "        sub = re.sub(r'_', \" \", sub)\n",
    "        # sub = re.sub(r'([A-Z]+)([A-Z][a-z0-9]+)', r'\\1 \\2', sub)\n",
    "        sub = re.sub(self.PATTERN_split, r'\\1 \\2', sub)\n",
    "        # sub = re.sub(r'([0-9]?[A-Z]+)', r' \\1', sub)\n",
    "        sub = re.sub(self.PATTERN_split_num, r' \\1', sub)\n",
    "        sub = re.sub(r'\\s+', \" \", sub).strip()\n",
    "        return sub\n",
    "\n",
    "    def uncamelize_by_stemming(self, name):\n",
    "        \"\"\"\n",
    "        uncamelzie the name and remove last num, eg. Student1->Student, JavaParser3->Java Parser\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # todo: improve this method to fix more situation, has some error for Path1->Path\n",
    "        name = re.sub(r'([0-9]+)$', '', name)\n",
    "        name = self.uncamelize(name)\n",
    "        if not name:\n",
    "            return None\n",
    "        sub = self.match_numer_first_and_middle(name)\n",
    "        if sub:\n",
    "            return sub\n",
    "        # self.merge_after_uncamelize_and_stemm(name)\n",
    "        return name\n",
    "\n",
    "    def match_numer_first_and_middle(self, name):\n",
    "        number_first = re.compile(r'(^[0-9]+[A-Z]+)', re.IGNORECASE).findall(name)\n",
    "        if number_first:\n",
    "            return number_first[0]\n",
    "        else:\n",
    "            number_middle = re.compile(r'([A-Z]+[24][A-Z]+)', re.IGNORECASE).findall(name.replace(\" \", \"\"))\n",
    "            if number_middle:\n",
    "                return number_middle[0]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def generate_aliases(self, qualified_name, include_simple_parent_name=False):\n",
    "        if not qualified_name:\n",
    "            return []\n",
    "\n",
    "        simple_name = self.simplify(qualified_name)\n",
    "        separate_name = self.uncamelize_from_simple_name(simple_name)\n",
    "        name_list = [simple_name, separate_name]\n",
    "\n",
    "        if include_simple_parent_name:\n",
    "            name_list.append(self.get_simple_name_with_parent(qualified_name))\n",
    "\n",
    "        name_list = [name for name in name_list if name]\n",
    "\n",
    "        return list(set(name_list))\n",
    "\n",
    "\n",
    "class ConceptElementNameUtil:\n",
    "    PATTERN_2_4 = re.compile(r'([A-Za-z])([24])([A-CE-Za-ce-z])')\n",
    "    PATTERN_split = re.compile(r'([A-Z]+)([A-Z][a-z0-9]+)')\n",
    "    PATTERN_split_num = re.compile(r'([0-9]?[A-Z]+)')\n",
    "\n",
    "    def get_simple_name_with_parent(self, name):\n",
    "        if not name:\n",
    "            return None\n",
    "        team_name = name.split(\"(\")[0]\n",
    "        split_names = team_name.split(\".\")\n",
    "        if len(split_names) <= 1:\n",
    "            return split_names[-1]\n",
    "\n",
    "        child = split_names[-1].strip()\n",
    "        parent = split_names[-2].strip()\n",
    "\n",
    "        return parent + \".\" + child\n",
    "\n",
    "    def simplify(self, name):\n",
    "        \"\"\"\n",
    "        get the simple name for class, method, field, eg. java.util.ArrayList->ArrayList\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        team_name = name.split(\"(\")[0]\n",
    "        simple_name = team_name.split(\".\")[-1].strip()\n",
    "\n",
    "        return simple_name\n",
    "\n",
    "    def uncamelize_from_simple_name(self, name):\n",
    "        \"\"\"\n",
    "        uncamel from simple name of one name, rg. java.util.ArrayList->Array List\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        simple_name = self.simplify(name)\n",
    "\n",
    "        return self.uncamelize(simple_name)\n",
    "\n",
    "    def uncamelize(self, name):\n",
    "        \"\"\"\n",
    "        uncamel one name\n",
    "        :param name: the camel styple name(include underline)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        # sub = re.sub(r'([A-Za-z])([24])([A-CE-Za-ce-z])', r'\\1 \\2 \\3', name).strip()\n",
    "        sub = re.sub(self.PATTERN_2_4, r'\\1 \\2 \\3', name).strip()\n",
    "        sub = re.sub(r'_', \" \", sub)\n",
    "        # sub = re.sub(r'([A-Z]+)([A-Z][a-z0-9]+)', r'\\1 \\2', sub)\n",
    "        sub = re.sub(self.PATTERN_split, r'\\1 \\2', sub)\n",
    "        # sub = re.sub(r'([0-9]?[A-Z]+)', r' \\1', sub)\n",
    "        sub = re.sub(self.PATTERN_split_num, r' \\1', sub)\n",
    "        sub = re.sub(r'\\s+', \" \", sub).strip()\n",
    "        return sub\n",
    "\n",
    "    def uncamelize_by_stemming(self, name):\n",
    "        \"\"\"\n",
    "        uncamelzie the name and remove last num, eg. Student1->Student, JavaParser3->Java Parser\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # todo: improve this method to fix more situation, has some error for Path1->Path\n",
    "        name = re.sub(r'([0-9]+)$', '', name)\n",
    "        name = self.uncamelize(name)\n",
    "        if not name:\n",
    "            return None\n",
    "        sub = self.match_numer_first_and_middle(name)\n",
    "        if sub:\n",
    "            return sub\n",
    "        # self.merge_after_uncamelize_and_stemm(name)\n",
    "        return name\n",
    "\n",
    "    def match_numer_first_and_middle(self, name):\n",
    "        number_first = re.compile(r'(^[0-9]+[A-Z]+)', re.IGNORECASE).findall(name)\n",
    "        if number_first:\n",
    "            return number_first[0]\n",
    "        else:\n",
    "            number_middle = re.compile(r'([A-Z]+[24][A-Z]+)', re.IGNORECASE).findall(name.replace(\" \", \"\"))\n",
    "            if number_middle:\n",
    "                return number_middle[0]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def generate_aliases(self, qualified_name, vocabulary=None, abbreviation=False):\n",
    "        if not qualified_name:\n",
    "            return []\n",
    "\n",
    "        simple_name = self.simplify(qualified_name)\n",
    "        of_names = self.deal_with_adj(simple_name)\n",
    "        separate_name = self.uncamelize_from_simple_name(simple_name)\n",
    "        name_deal_number = self.uncamelize_by_stemming(simple_name)\n",
    "        combined_name = simple_name.lower().replace(\"-\", \"\").replace(\"\\\\\", \"\").replace(\" \", \"\")\n",
    "\n",
    "        name_list = [simple_name, separate_name, name_deal_number, combined_name]\n",
    "        name_list.extend(of_names)\n",
    "\n",
    "        if abbreviation:\n",
    "            result = self.generate_all_abbreviation_names(separate_name, vocabulary)\n",
    "            name_list.extend(result)\n",
    "\n",
    "        name_list = [name for name in name_list if name]\n",
    "\n",
    "        return list(set(name_list))\n",
    "\n",
    "    def generate_all_abbreviation_names(self, separate_name, vocabulary):\n",
    "        result = []\n",
    "        part_abbreviation = []\n",
    "        full_abbreviation_name = self.get_abbreviation(separate_name)\n",
    "        abbreviation_name = self.get_abbreviation(separate_name, full_link=False)\n",
    "        if vocabulary != None:\n",
    "            part_abbreviation = self.get_part_abbreviation(separate_name, vocabulary)\n",
    "        result.append(full_abbreviation_name)\n",
    "        result.append(abbreviation_name)\n",
    "        result.extend(part_abbreviation)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def deal_with_adj(self, name):\n",
    "        result = []\n",
    "        seperate_words = [\" of \", \" Of \"]\n",
    "\n",
    "        # A of B type\n",
    "        for s_w in seperate_words:\n",
    "            if s_w in name:\n",
    "                words = name.split(s_w)\n",
    "                if len(words) != 2:\n",
    "                    continue\n",
    "                child = words[0]\n",
    "                parent = words[1]\n",
    "                result.append((parent + \" \" + child).replace(\"  \", \" \"))\n",
    "\n",
    "        seperate_words = [\"'s \", \"' \"]\n",
    "\n",
    "        # A's B => A B\n",
    "        for s_w in seperate_words:\n",
    "            if s_w in name:\n",
    "                words = name.split(s_w)\n",
    "                if len(words) != 2:\n",
    "                    continue\n",
    "                parent = words[0]\n",
    "                child = words[1]\n",
    "\n",
    "                result.append((parent + \" \" + child).replace(\"  \", \" \"))\n",
    "                result.append((child + \" of \" + parent).replace(\"  \", \" \"))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_abbreviation(self, separate_name, full_link=True):\n",
    "        separate_name_list = separate_name.lower().split(\" \")\n",
    "        if len(separate_name_list) <= 1:\n",
    "            return separate_name\n",
    "        if full_link:\n",
    "            abbreviation_list = [name[0].upper() for name in separate_name_list]\n",
    "            return \"\".join(abbreviation_list)\n",
    "        else:\n",
    "            abbreviation_list = [name[0].upper() for name in separate_name_list if\n",
    "                                 name not in [\"of\", \"the\", \"this\", \"a\", \"that\"]]\n",
    "            return \"\".join(abbreviation_list)\n",
    "\n",
    "    def get_part_abbreviation(self, separate_name, vocabulary):\n",
    "        \"\"\"\n",
    "        :param separate_name: term name\n",
    "        :param vocabulary: a list of prase\n",
    "        :return: a list of abbreviation\n",
    "        \"\"\"\n",
    "        separate_name_list = separate_name.split(\" \")\n",
    "        return_list = []\n",
    "        if len(separate_name_list) > 1:\n",
    "            for index in range(len(separate_name_list) - 1):\n",
    "                for inner_index in range(index + 1, len(separate_name_list)):\n",
    "                    prase = \" \".join(separate_name_list[index:inner_index + 1])\n",
    "                    if prase in vocabulary:\n",
    "                        abbreviation_list = [name[0].upper() for name in separate_name_list[index:inner_index + 1]]\n",
    "                        abbreviation = separate_name.replace(prase, \"\".join(abbreviation_list))\n",
    "                        return_list.append(abbreviation)\n",
    "            return return_list\n",
    "        else:\n",
    "            return [separate_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "class EntityExtractor(object):\n",
    "    \"\"\"\n",
    "    extract useful information from text, eg. HTML text, comment style text, normal text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = SpacyNLPFactory.create_spacy_nlp_for_domain_extractor()\n",
    "        self.pattern = re.compile(r\"NP_\\w+ of NP_\\w+\")\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.stopwords.append(\"-PRON-\")\n",
    "        self.stopwords = set(self.stopwords)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.code_patterns = [\n",
    "            re.compile(r'^(?P<ELE>[a-zA-Z0-9_]*[a-z0-9][A-Z][a-z]+[a-zA-Z0-9_]*)(<.*>)?$'),\n",
    "            re.compile(r'^(?P<ELE>[a-zA-Z0-9_\\.<>]+)\\([a-zA-Z0-9_\\,.<>)]*?$'),\n",
    "            re.compile(r'^(?P<ELE>[a-zA-Z]{2,}(\\.[a-zA-Z0-9_]+)+)(<.*>)?$'),\n",
    "        ]\n",
    "\n",
    "        self.camel_cache = {}\n",
    "        self.CODE_NAME_UTIL = CodeElementNameUtil()\n",
    "\n",
    "    def uncamelize(self, camel_case):\n",
    "        if camel_case in self.camel_cache:\n",
    "            return self.camel_cache[camel_case]\n",
    "        sub = self.CODE_NAME_UTIL.uncamelize_by_stemming(camel_case)\n",
    "        self.camel_cache[camel_case] = sub\n",
    "        return sub\n",
    "\n",
    "    def extract_from_sentence(self, sent):\n",
    "        \"\"\"\n",
    "        extract concept from one sentence.\n",
    "        :param sent:\n",
    "        :return: a set of concepts.\n",
    "        \"\"\"\n",
    "        code_elements = self.extract_code_element(sent)\n",
    "\n",
    "        domain_terms = set()\n",
    "        doc = self.nlp(sent)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            # print(\"chunk: \", chunk.text)\n",
    "            chunk = self.clean_chunk(chunk)\n",
    "            # print(\"cleaned chunk:\", chunk)\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            if len(chunk) == 1 and self.is_word_common(chunk.text):\n",
    "                continue\n",
    "            if chunk.text in code_elements:\n",
    "                continue\n",
    "            # domain_terms.add(self.__chunk_lemmatize(chunk))\n",
    "            domain_terms.update(self.extract_abbreviation_from_chunk(chunk))\n",
    "            domain_terms.update(self.extract_NNPs_from_chunk(chunk))\n",
    "        domain_terms.update(self.extract_np_of_np(doc))\n",
    "        # print('sent: ' + sent)\n",
    "        # print('result: ', result)\n",
    "        domain_terms = self.__post_process(domain_terms)\n",
    "        return domain_terms, code_elements\n",
    "\n",
    "    def extract_code_element(self, sent):\n",
    "        elements = set()\n",
    "        for word in sent.split():\n",
    "            word = word.lstrip(\"#(\").rstrip(\",;.!?\")\n",
    "            # print(word)\n",
    "            for index, pattern in enumerate(self.code_patterns):\n",
    "                search_rs = pattern.search(word)\n",
    "                if search_rs is not None and search_rs.group(\"ELE\"):\n",
    "                    # print(index, pattern, search_rs.group(\"ELE\"))\n",
    "                    elements.add(search_rs.group(\"ELE\"))\n",
    "                elif index == len(self.code_patterns) - 1:\n",
    "                    p = re.compile(r'([a-z]|\\d)([A-Z])')\n",
    "                    if p.search(word) is not None:\n",
    "                        # print(\"camel:\", word)\n",
    "                        elements.add(word)\n",
    "        return elements\n",
    "\n",
    "    def extract_np_of_np(self, doc):\n",
    "        result = set([])\n",
    "        sentence_text = doc[:].lemma_\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_arr = []\n",
    "            chunk = self.clean_chunk(chunk)\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            for token in chunk:\n",
    "                chunk_arr.append(token.lemma_)\n",
    "            chunk_lemma = \" \".join(chunk_arr)\n",
    "            # print(\"chunk_lemma\", chunk_lemma)\n",
    "            replacement_value = \"NP_\" + \"_\".join(chunk_arr)\n",
    "            # print(\"replacement_value\", replacement_value)\n",
    "            sentence_text = sentence_text.replace(chunk_lemma, replacement_value)\n",
    "        # print(\"sentence_text\", sentence_text)\n",
    "        matches = re.findall(self.pattern, sentence_text)\n",
    "        if len(matches) > 0:\n",
    "            # print('matched: ', matches)\n",
    "            for m in matches:\n",
    "                result.add(m.replace(\"NP_\", \"\").replace(\"_\", \" \"))\n",
    "        return result\n",
    "\n",
    "    def clean_chunk(self, chunk):\n",
    "        \"\"\"\n",
    "        remove the stopwords, digits and pronouns at the start of the chunk.\n",
    "        pass the result which contains invalid symbol.\n",
    "        :param chunk:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk.text.lower() in self.stopwords:\n",
    "            return []\n",
    "        while len(chunk) > 1:\n",
    "            start_token = chunk[0]\n",
    "            if start_token.text.lower() in self.stopwords or start_token.text.isdigit() or start_token.tag_ == 'PRP':\n",
    "                chunk = chunk[1:]\n",
    "            else:\n",
    "                break\n",
    "        if len(chunk) == 1:\n",
    "            start_token = chunk[0]\n",
    "            if start_token.text.lower() in self.stopwords or start_token.text.isdigit() or start_token.tag_ == 'PRP':\n",
    "                return []\n",
    "        if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9\\' -]*[a-zA-Z0-9]$', chunk.text):\n",
    "            return []\n",
    "        return chunk\n",
    "\n",
    "    def is_word_common(self, word):\n",
    "        \"\"\"\n",
    "        check if the word is common word.\n",
    "        :param word:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if word in self.stopwords:\n",
    "            return True\n",
    "        if re.match(r'[a-zA-Z]+[a-zA-Z]$', word):\n",
    "            word = self.lemmatizer.lemmatize(word, pos='n')\n",
    "            synset = wn.synsets(word)\n",
    "            if len(synset) > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def extract_abbreviation_from_chunk(self, chunk):\n",
    "        result = set([])\n",
    "        for token in chunk:\n",
    "            if re.match(r'[A-Z]{2,}[0-9]*$', token.text):\n",
    "                result.add(token.text)\n",
    "        return result\n",
    "\n",
    "    def extract_NNPs_from_chunk(self, chunk):\n",
    "        result = set([])\n",
    "        p = 0\n",
    "        while p < (len(chunk) - 1):\n",
    "            if chunk[p].tag_.startswith('NNP'):\n",
    "                for i in range(p + 1, len(chunk)):\n",
    "                    if not chunk[i].tag_.startswith('NNP'):\n",
    "                        t_w = chunk[p:i]\n",
    "                        p = i\n",
    "                        if len(t_w) > 1:\n",
    "                            result.add(self.__chunk_lemmatize(t_w))\n",
    "                        break\n",
    "                    elif i == len(chunk) - 1:\n",
    "                        t_w = chunk[p:]\n",
    "                        p = i\n",
    "                        if len(t_w) > 1:\n",
    "                            result.add(self.__chunk_lemmatize(t_w))\n",
    "                        break\n",
    "            else:\n",
    "                p = p + 1\n",
    "        return result\n",
    "\n",
    "    def __chunk_lemmatize(self, chunk):\n",
    "        \"\"\"\n",
    "        lemmatize the last word of chunk.\n",
    "        :param chunk:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        word = self.lemmatizer.lemmatize(chunk.text, pos='n')\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __post_process(self, result):\n",
    "        new_result = set([])\n",
    "        for item in result:\n",
    "            if len(item) == 1 or item.isdigit():\n",
    "                continue\n",
    "            new_result.add(item)\n",
    "        return new_result\n",
    "\n",
    "    def extract_from_comment(self, comment):\n",
    "        \"\"\"\n",
    "        extract domain_terms, code_elements from comment text\n",
    "        :param comment:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        comment = re.sub(r'\\s+', ' ', comment.strip().strip(\"/*\").strip())\n",
    "        if len(comment) == 0:\n",
    "            return set(), set()\n",
    "        domain_terms, code_elements = self.extract_from_sentence(comment)\n",
    "        return domain_terms, code_elements\n",
    "\n",
    "    def extract_from_html(self, html):\n",
    "        terms = set()\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        tts = {tt.get_text() for tt in soup.findAll(\"tt\")}\n",
    "        terms.update({tt for tt in tts if len(tt.split()) <= 3})\n",
    "        sent = soup.get_text()\n",
    "        sent = re.sub(r'\\s+', ' ', sent.strip().strip(\"/*\").strip())\n",
    "        domain_terms, code_elements = self.extract_from_sentence(sent)\n",
    "        for term in domain_terms:\n",
    "            terms.add(term)\n",
    "        return terms, code_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = EntityExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(set(), {'file.setExcutable', 'file.setExcutable(arg)'})"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "terms = set()\n",
    "codes = set()\n",
    "for sentence in sentences:\n",
    "    domain_terms, code_elements = extractor.extract_from_sentence(sentence)\n",
    "    terms.update(domain_terms)\n",
    "    codes.update(code_elements)\n",
    "terms, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ncode element可以被抽取出来，给图谱注入新的关系...\\n这个可以加在之后的工作内容里，见上方尝试\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "'''\n",
    "code element可以被抽取出来，给图谱注入新的关系...\n",
    "这个可以加在之后的工作内容里，见上方尝试\n",
    "至此初步的domain term抽取工作算是完成了，后面怎么fuse跟链接wiki之后再说...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pheolon_kernel",
   "language": "python",
   "name": "pheolon_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}