{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "new_so_kernal",
   "display_name": "new_so_kernal"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "先尝试纯文本化方法\n",
    "'''\n",
    "WIKIPEDIA_CONCEPT_STORE_PATH = 'C:/workspace/SOworkspace/data/wikipedia_concepts/text_content/'\n",
    "stemmed_wiki_descriptions_path = os.path.join(WIKIPEDIA_CONCEPT_STORE_PATH, 'stemmed/stemmed.txt')\n",
    "#先把所有wikipedia的概念存起来\n",
    "pure_wiki_concept_word_path = os.path.join(WIKIPEDIA_CONCEPT_STORE_PATH, 'wikipedia_concept_words.pkl')\n"
   ]
  },
  {
   "source": [
    "#这个cell别跑了，在nb里跑太慢，我放在py里跑了\n",
    "\"\"\"\n",
    "with open(stemmed_wiki_descriptions_path, 'r', encoding='utf-8') as rf, open(pure_wiki_concept_word_path, 'wb') as wf:\n",
    "    wiki_concept_words = []\n",
    "    while True:\n",
    "        text_line = rf.readline()\n",
    "        if text_line:\n",
    "            concept_word = str(text_line).split('|||')[0].strip()\n",
    "            wiki_concept_words.append(concept_word)\n",
    "            print(\"\\r\",len(wiki_concept_words), concept_word,end=\"\",flush=True)\n",
    "        else:\n",
    "            break\n",
    "    pickle.dump(wiki_concept_words, wf)\n",
    "\"\"\""
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pure_wiki_concept_word_path, 'rb') as rf:\n",
    "    wiki_concept_words = pickle.load(rf)\n",
    "    len(wiki_concept_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_concept_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javadoc_description_path = 'C:/workspace/SOworkspace/data/apidoc_description/javadoc_descriptions.pkl'\n",
    "with open(javadoc_description_path, 'rb') as rf:\n",
    "    javadoc_description = pickle.load(rf)"
   ]
  },
  {
   "source": [
    "'''\n",
    "这个本机跑太慢了，就放到服务器上跑了。。\n",
    "跑完的文件放在了C:\\workspace\\SOworkspace\\data\\wikipedia_concepts\\text_content\\javadoc_wiki_concepts_pure_text.pkl文件夹里\n",
    "\n",
    "javadoc_wiki_concept_path = 'C:/workspace/SOworkspace/data/apidoc_description/javadoc_wiki_concepts_pure_text.pkl'\n",
    "from tqdm import tqdm\n",
    "javadoc_concepts = {}\n",
    "for concept, description in tqdm(javadoc_description.items()):\n",
    "    temp_words = []\n",
    "    for word in wiki_concept_words:\n",
    "        if word in description:\n",
    "            temp_words.append(word)\n",
    "    javadoc_concepts[concept] = temp_words\n",
    "with open(javadoc_wiki_concept_path, 'wb') as wf:\n",
    "    pickle.dump(javadoc_concepts, wf)\n",
    "'''"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#好，看看跑出来的直接在description里识别文本的识别结果怎么样\n",
    "with open(r'C:\\workspace\\SOworkspace\\data\\wikipedia_concepts\\text_content\\javadoc_wiki_concepts_pure_text.pkl', 'rb') as rf:\n",
    "    javadocdesc_wikiconcept_puretext_match = pickle.load(rf)\n",
    "javadocdesc_wikiconcept_puretext_match_list = list(javadocdesc_wikiconcept_puretext_match.items())\n",
    "\n",
    "javadocdesc_wikiconcept_puretext_match_list[12345]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javadocdesc_wikiconcept_puretext_match_list = list(javadocdesc_wikiconcept_puretext_match.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉空格\n",
    "desc_wiki_text_match_list_non_white = [(k, [i for i in v if len(i) > 2]) for k, v in javadocdesc_wikiconcept_puretext_match_list]\n",
    "desc_wiki_text_match_list_non_white[13457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''试着算一下tf-idf提取词频比较高的词汇'''\n",
    "from gensim import corpora\n",
    "#生成api和描述分别的列表，用描述列表做tf-idf训练。到时候再和api列表做对应\n",
    "api_names = []\n",
    "descs = []\n",
    "for k, v in desc_wiki_text_match_list_non_white:\n",
    "    api_names.append(k)\n",
    "    descs.append(v)\n",
    "descs[0: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionay = corpora.Dictionary(descs)\n",
    "desc_corpus = [dictionay.doc2bow(text) for text in descs]\n",
    "desc_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(desc_corpus)\n",
    "tfidf.save(r'C:\\workspace\\SOworkspace\\data\\cache\\javadoc_description_wikiwords.tfidf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dictionay.token2id\n",
    "id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "api/java.base/java/util/Calendar.html#NOVEMBER\n=====\n[('ego', 0.41907358702457753), ('mon', 0.32930740796692987), ('event', 0.29366648096222003), ('even', 0.27753431805579765), ('oria', 0.2479148020387051), ('calendar', 0.242685009766123), ('mont', 0.2189550797563686), ('month', 0.2189550797563686), ('year', 0.21218309183773393), ('leve', 0.2110005856434952), ('gor', 0.17608155440633194), ('dar', 0.1665797765032846), ('nda', 0.16039185782601778), ('field', 0.1588381133489189), ('ear', 0.15544240911602447), ('indic', 0.1549156186120962), ('dic', 0.14225174556457393), ('indi', 0.14155394243509964), ('eve', 0.10293486151870439), ('ele', 0.10235513085929446), ('tin', 0.08695521896887032), ('cat', 0.08415072835706143), ('cal', 0.07987346474800702), ('ati', 0.0516091753409838), ('and', 0.04933550084137336), ('ing', 0.03937574756681835)]\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_desc(desc: list):\n",
    "    bow = dictionay.doc2bow(desc)\n",
    "    desc_tf_idf = tfidf[bow]\n",
    "    return sorted([(id2word[id], vec) for id, vec in desc_tf_idf], key=lambda v : v[1], reverse=True)\n",
    "\n",
    "def show_desc_tfidf_status(desc_id):\n",
    "    print(api_names[desc_id])\n",
    "    print('=====')\n",
    "    print(get_tfidf_desc(descs[desc_id][0:30]))\n",
    "\n",
    "show_desc_tfidf_status(12313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}